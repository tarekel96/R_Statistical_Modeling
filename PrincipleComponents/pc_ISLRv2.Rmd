---
title: "Principle Components"
author: "Tarek El-Hajjaoui"
date: "2023-05-04"
output: html_document
---

## ISLR v2 Chapter 6: Problem 9 - College Applications

```{r, message=FALSE}
library(caret) # createDataPartition
library(leaps) # regsubsets - best subset model
library(glmnet) # glmnet - lasso/ridge regression
library(pls) # pcr, pls
library(dplyr) # arrange
```

```{r}
file_path = '/Users/Tarek/Documents/UCI_MDS_Coding/Stats211P/R_Statistical_Modeling/ISLRv2_Datasets/College.csv'
df_college <- read.csv(file_path, row.names = NULL, header = TRUE)
df_college <- na.omit(df_college)
# drop column X from the dataframe
df_college$X <- NULL
```

```{r}
categorical_cols <- c('Private')
df_college[categorical_cols] <- lapply(df_college[categorical_cols], as.factor)
```

```{r}
str(df_college) # ensure data types correct now
colSums(is.na(df_college)) # count number of nulls in each column
nrow(df_college)
```

### 9. a)

Split the data set into a training set and a test set.
- Y = Apps, number of applications.
- X = All of the other features.
```{r}
set.seed(0) # set random seed for reproducibility
# split the dataframe into train and test sets
train_indices <- createDataPartition(df_college$Apps, p = 0.7, list = FALSE)
train_college <- df_college[train_indices, ]
train_y <- data.matrix(subset(train_college, select=c('Apps')))
train_X <- data.matrix(train_college[,!names(train_college) %in% c("Apps")])
dim(train_X);dim(train_y) # checking dims of split data
test_college <- df_college[-train_indices, ]
test_y <- data.matrix(subset(test_college, select=c('Apps')))
test_X <- data.matrix(test_college[,!names(test_college) %in% c("Apps")])
dim(test_X);dim(test_y) # checking dims of split data
```

### 9. b)

Fit a linear model using least squares on the training set, and report the test error obtained.
```{r}
# fit a linear model using least squares on the training set
lr.mod <- lm(Apps ~ ., data = train_college)
# predict the response variable on the test set
lr.pred <- predict(lr.mod, newdata = test_college)
# calculate the test error (mean squared error)
lr.MSE <- mean((lr.pred - test_college$Apps)^2)
lr.MSE
```

### 9. c)

Fit a ridge regression model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained.

```{r}
# set up training control object for 10-fold cross-validation
ridge.cv.out <- cv.glmnet(train_X, train_y, alpha = 0)

# plot the hyperparmater tuning cross-validation
plot(ridge.cv.out)

# get best lambda
ridge.bestlam <- ridge.cv.out$lambda.min

ridge.mod <- cv.glmnet(train_X, train_y, alpha = 0)

# make predictions with the best lambda
ridge.pred <- predict(ridge.mod, s = ridge.bestlam, newx = test_X)

# calculate test error (mean squared error)
ridge.MSE <- mean((ridge.pred - test_y)^2)

ridge.MSE
```

### 9. d)

Fit a lasso model on the training set, with $\lambda$ chosen by cross- validation. Report the test error obtained, along with the number of non-zero coefficient estimates.
```{r}
# set up training control object for 10-fold cross-validation
lasso.cv.out <- cv.glmnet(train_X, train_y, alpha = 1)

# plot the hyperparmater tuning cross-validation
plot(lasso.cv.out)

# get best lambda
lasso.bestlam <- lasso.cv.out$lambda.min

lasso.mod <- cv.glmnet(train_X, train_y, alpha = 1)

# make predictions with the best lambda
lasso.pred <- predict(lasso.mod, s = lasso.bestlam, newx = test_X)

# calculate test error (mean squared error)
lasso.MSE <- mean((lasso.pred - test_y)^2)

lasso.MSE
```

### 9. e)

Fit a PCR model on the training set, with M chosen by cross- validation. Report the test error obtained, along with the value of M selected by cross-validation.
```{r}
options(scipen=999) # disable scientifit notation

# perform PCR on the training data with CV
pcr.mod <- pcr(Apps ~ ., data = train_college, scale = TRUE,
    validation = "CV") # cross validation

# view CV plot to choose ncomp
validationplot(pcr.mod, val.type = "MSEP")
```

In the plot above, it can be observed there is a big drop in MSE after 3-4 components are used. There appears to be flattening right after 5 components. That being said, 6 principle components should be sufficient in making predictions.

```{r}
# make predictions
pcr.pred <- predict(pcr.mod, test_college, ncomp = 6)

# calculate test error (mean squared error)
pcr.MSE <- mean((pcr.pred - test_college$Apps)^2)

pcr.MSE
```

### 9. f)

Fit a PLS model on the training set, with M chosen by cross- validation. Report the test error obtained, along with the value of M selected by cross-validation.
```{r}
options(scipen=999) # disable scientifit notation

# perform PLS on the training data with CV
pls.mod <- plsr(Apps ~ ., data = train_college, scale = TRUE,
    validation = "CV") # cross validation

# view CV plot to choose ncomp
validationplot(pls.mod, val.type = "MSEP")
```

In the plot above, it can observed there is a big drop in MSE after 2-3 components are used. There appears to be flattening right after 5 components. That being said, 6 principle components should be sufficient in making predictions.

```{r}
# make predictions
pls.pred <- predict(pls.mod, test_college, ncomp = 6)

# calculate test error (mean squared error)
pls.MSE <- mean((pls.pred - test_college$Apps)^2)

pls.MSE
```

### 9. g)
Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?
```{r}
MSE_results <- c(lr.MSE, ridge.MSE, lasso.MSE, pcr.MSE, pls.MSE)
MSE_labels <- c("lr.MSE", "ridge.MSE", "lasso.MSE", "pcr.MSE", "pls.MSE")
df_MSE <- data.frame(Model = MSE_labels, Test_Error = MSE_results)
arrange(df_MSE, desc(Test_Error))
```

- With all of the different approaches, there appears to be a high amount of MSE which suggests that we cannot strongly/accurately predict the number of college applications received with the given dataset.
- It is expected for the PCR and PLS models to have higher MSE values than the other models because they are intended to reduce the number of predictor variables. PLS did perform significantly better in terms of MSE which suggests that the dataset has highly correlated predictors or a lot of noise/outliers.
- Ridge regression performs the best in terms of having the lowest MSE, which could be for couple reasons. One possibility is that the true coefficients of some features are small and so lasso elminates them entirely, while ridge regression keeps these coefficients.

## ISLR v2 Chapter 6: Problem 11 - Bostom Crime Rate

```{r}
file_path = '/Users/Tarek/Documents/UCI_MDS_Coding/Stats211P/R_Statistical_Modeling/ISLRv2_Datasets/Boston.csv'
df_boston <- read.csv(file_path, row.names = NULL, header = TRUE)
df_boston <- na.omit(df_boston)
# drop column X from the dataframe
df_boston$X <- NULL
```

```{r}
categorical_cols <- c('chas', 'rad')
df_boston[categorical_cols] <- lapply(df_boston[categorical_cols], as.factor)
```

```{r}
str(df_boston) # ensure data types correct now
colSums(is.na(df_boston)) # count number of nulls in each column
nrow(df_boston)
```
### 11. a)

Try out some of the regression methods explored in this chapter, such as best subset selection, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider.
```{r}
set.seed(0) # set random seed for reproducibility

# split the dataframe into train and test sets
train_indices <- createDataPartition(df_boston$crim, p = 0.7, list = FALSE)
train_boston <- df_boston[train_indices, ]
train_y <- data.matrix(subset(train_boston, select=c('crim')))
train_X <- data.matrix(train_boston[,!names(train_boston) %in% c("crim")])
dim(train_X);dim(train_y) # checking dims of split data
test_boston <- df_boston[-train_indices, ]
test_y <- data.matrix(subset(test_boston, select=c('crim')))
test_X <- data.matrix(test_boston[,!names(test_boston) %in% c("crim")])
dim(test_X);dim(test_y) # checking dims of split data
y_true <- test_boston$crim
```

Trying different models: [pcr, best subset, ridge, lasso]

PCR
```{r}
# perform PCR on the training data with CV
pcr.mod <- pcr(crim ~ ., data = train_boston, scale = TRUE,
    validation = "CV") # cross validation
# view CV plot to choose ncomp
validationplot(pcr.mod, val.type = "MSEP")
```

```{r}
# make predictions
pcr.pred <- predict(pcr.mod, test_boston, ncomp = 5)
# calculate test error (mean squared error)
pcr.MSE <- mean((y_true - pcr.pred)^2)
```

Best Subset
```{r}
# Best Subset - paramater tuning
k <- 10 # 10-Fold CV
n <- nrow(train_boston)
folds <- sample(rep(1:k, length=n))
cv.errors <- matrix(NA, k, 13, dimnames = list(NULL, paste(1:13)))
test_boston <- na.omit(test_boston)
# perform best subset selection within each of the k training sets
for (j in 1:k) {
  best.fit <- regsubsets(crim ~ .,
                         data = train_boston[folds != j, ],
                         nvmax=13)
  # preparing the test matrix
  test.mat <- model.matrix(crim ~ ., data = train_boston[folds == j, ])
  y_hat <- train_boston$crim[folds == j]
  for (i in 1:13) {
    best.fit.coefi <- coef(best.fit, i)
    # make predictions
    pred <- test.mat[, names(best.fit.coefi)] %*% best.fit.coefi
    # save k-fold mse
    cv.errors[j, i] <- mean((y_hat - pred)^2)
  }
}
# mean for each subset model with i number of features
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors

par(mfrow = c(1, 1))
plot(mean.cv.errors, type = "b", xlab = "Number of Featuers", ylab="Validation MSE")

# Best Subset - model building with optimal number of features, 13
# fit a best subset model training set
regfit.mod <- regsubsets(crim ~ ., data = train_boston, nvmax = 13)
# preparing the test matrix
test.mat <- model.matrix(crim ~ ., data = test_boston)
regfit.coefi <- coef(regfit.mod, 13)
# predict the response variable on the test set
regfit.pred <- test.mat[, names(regfit.coefi)] %*% regfit.coefi
# calculate the test error (mean squared error)
regfit.MSE <- mean((y_true - regfit.pred)^2)
regfit.MSE
```

```{r}
# Ridge
# set up training control object for 10-fold cross-validation
ridge.cv.out <- cv.glmnet(train_X, train_y, alpha = 0)

# plot the hyperparmater tuning cross-validation
plot(ridge.cv.out)

# get best lambda
ridge.bestlam <- ridge.cv.out$lambda.min

ridge.mod <- cv.glmnet(train_X, train_y, alpha = 0)

# make predictions with the best lambda
ridge.pred <- predict(ridge.mod, s = ridge.bestlam, newx = test_X)

# calculate test error (mean squared error)
ridge.MSE <- mean((ridge.pred - test_y)^2)

ridge.MSE

# Lasso
# set up training control object for 10-fold cross-validation
lasso.cv.out <- cv.glmnet(train_X, train_y, alpha = 1)

# plot the hyperparmater tuning cross-validation
plot(lasso.cv.out)

# get best lambda
lasso.bestlam <- lasso.cv.out$lambda.min

lasso.mod <- cv.glmnet(train_X, train_y, alpha = 1)

# make predictions with the best lambda
lasso.pred <- predict(lasso.mod, s = lasso.bestlam, newx = test_X)

# calculate test error (mean squared error)
lasso.MSE <- mean((lasso.pred - test_y)^2)

lasso.MSE
```

```{r}
MSE_results <- c(regfit.MSE, ridge.MSE, lasso.MSE, pcr.MSE)
MSE_labels <- c("regfit.MSE", "ridge.MSE", "lasso.MSE", "pcr.MSE")
df_MSE <- data.frame(Model = MSE_labels, Test_Error = MSE_results)
arrange(df_MSE, desc(Test_Error))
```

All of the methods above were trained on 70% of the original Boston dataset by splitting the dataset via createDataPartition from the caret library. Each model's performance was tested via the test set (30% of the orignal dataset). The results can be summarized above. A description of each approach:

  * PCR: Cross-validation was on performed for the number of features, $M$. The validation plot shows the MSE for models with the $x_i$ number of components. We can observe there is a large drop in validation MSE around 2 features and then another big drop around 4 - 5 features. For the sake of testing the effectiveness of dimension reduction, only 5 features were chosen in the final pcr model to compare with the later models in terms of test set performance. The model with 5 features was used to predict y, crime rate in Boston. The test MSE was calculated based on these predictions. 

  * Best Subset: A subset selection of all 13 predictor variables was performed via the regsubsets method. 10-fold validation was performed to choose the optimal number of components for the subset model. The best subset model was 13 features which tells us that all 13 features were helpful in predicting crime rate. The final subset model was created with all 13 features to create predictions. The test MSE was calculated based on these predictions. 
  
  * Ridge & Lasso: Ridge and Lasso modeling was done via very similar methods. 10-fold cross-validation was performed to derive the optimal lambda value for each model. Then a model for each was made with its respective best lambda value to make predictions on the test dataset. The test MSE was calculated based on these predictions. 
  
### 11. b)

Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, cross- validation, or some other reasonable alternative, as opposed to using training error.

- The test MSE dataframe printed above demonstrates that the models performed similarly. Ridge and lasso regression performed worse than the other 2 methods. This could be because multicollinearity, perhaps some features are dependent on other features and the ridge/lasso regression are wrongly diminishing/elminating feature coeifficients which result in weaker predictions. The pcr model with only 5 predictions was able to attain better performane than both lasso and ridge. This could be because pcr handles multicollinearity better than lasso and ridge. Another possibility is that pcr is able to capture nonlinear relationships which ridge/lasso cannot because of their assumptions. The subset model with 13 features was able to perform the best. This model used all of the features to do its predictions. Although it was able to perform better than pcr, the pcr model is the best choice model because it is simpler model with similar prediction power. This is more computationally efficient and by being a simpler model, it is less likely to overfit to the training data and can possibly perform better in other unseen data (besides the test set used here) than the other models. 

### 11. c)

Does your chosen model involve all of the features in the data set? Why or why not?

- The chosen model in part b, pcr, only has 5 features. The idea of 5 features came from validation plot of the pcr model. It was observed that a model with 5 components was able to make predictions with low validation MSE. Since the pcr model with 5 components performs nearly as good as the rest of the models, it was chosen. In general if you can have a model B with almost equal prediction power but half as many features as model A, then model B should be chosen because it is a lot more computationally efficient and less likely to overfit since it is a simpler model.