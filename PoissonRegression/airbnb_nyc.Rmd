---
title: "Generalized Linear Models"
author: "Tarek El-Hajjaoui"
date: "2023-04-23"
output: html_document
---

```{r, echo=FALSE, results="hide", warning=FALSE, message=FALSE}
# library
library(tidyverse)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(kableExtra)
library(knitr)
```

## AirBnB

### Q 4.11.3.1 Airbnb in NYC
Perform an EDA, build a model, and interpret model coefficients to describe variation in the number of reviews (a proxy for the number of rentals, which is not available) as a function of the variables provided. Don’t forget to consider an offset, if needed.

```{r}
# Loading the dataset
file_path = './NYCairbnb.csv'
df = read.csv(file_path, header = TRUE)
```

```{r}
# View dataframe summary
str(df)
```

####  Q 4.11.3.1 Airbnb in NYC - Explanatory Data Analysis (EDA)

Preprocessing - transforming data types
```{r}
# Transforming categorical columns to factor data type.
categorical_cols <- c('room_type', 'bathrooms', 'bedrooms', 'review_scores_cleanliness', 'review_scores_location', 'review_scores_value', 'instant_bookable')
df[categorical_cols] <- lapply(df[categorical_cols], as.factor)

# Transforming date columns
date_cols <- c('last_scraped', 'host_since')
df[date_cols] <- lapply(df[date_cols], as.Date)
```

**Checking if the data justifies a Poisson regression model.**

Plotting the $Y$, number of reviews.
```{r}
ggplot(df, aes(number_of_reviews)) +
  geom_histogram(binwidth = .25, color = "black", fill = "white") +
  xlim(0, 100) +
  xlab("Number of reviews") +
  ylab("Count of the number of reviews")

max(df[['number_of_reviews']])
```

**Note:** X-axis has been condensed for visual reasons, however, there are some (not many) AirBnB observations with number of reviews ranging from 100-421.

- Number of reviews appears to follow a Poisson distribution given by the right-skewed histogram above. As the number of reviews increases, the count of the number of reviews decreases.

Checking for the assumption of $\mu = \sigma^2$ (mean = variance).
```{r, echo=FALSE, message = FALSE}

# Mean = Variance [group by review_scores_cleanliness]
table1<- df  %>% group_by(review_scores_cleanliness)  %>% 
  summarise(mnNum= mean(number_of_reviews),varNum=var(number_of_reviews),n=n())
k1 <- kable(table1, booktabs=T, 
      caption="Compare mean and variance of the number of reviews for the review score cleaniness groups.",col.names = c("review_scores_cleanliness", "Mean", "Variance", "n")) %>% kable_styling(full_width = F)

# Mean = Variance [group by review_scores_location]
table2<- df  %>% group_by(review_scores_location)  %>% 
  summarise(mnNum= mean(number_of_reviews),varNum=var(number_of_reviews),n=n())
k2 <- kable(table2, booktabs=T, 
      caption="Compare mean and variance of the number of reviews for the review score location groups.",col.names = c("review_scores_location", "Mean", "Variance", "n"))%>% kable_styling(full_width = F)

# Mean = Variance [group by review_scores_value]
table3<- df  %>% group_by(review_scores_value)  %>% 
  summarise(mnNum= mean(number_of_reviews),varNum=var(number_of_reviews),n=n())
k3 <- kable(table3, booktabs=T, 
      caption="Compare mean and variance of the number of reviews for the review score value groups.",col.names = c("review_scores_value", "Mean", "Variance", "n")) %>% kable_styling(full_width = F)

# display mean and variance tables of features
k1
k2
k3
```
There is some evidence of a violation of the mean=variance assumption. It appears that the variance is smaller than the mean for lower number of reviews, while the variance is greater than the mean for higher number of reviews. 

The Poisson regression model implies that log($\lambda_i$) is a linear function of feature variables; i.e., $log(\lambda_i)=\beta_0+\beta_i\textrm{feature}_i$. Therefore, to check the linearity assumption for Poisson regression, we would like to plot log($\lambda_i$) by $\beta_i$.

```{r}
# Remove rows where x is null
df_no_nulls <- df[complete.cases(df), ]

sumStats <- df_no_nulls %>% group_by(review_scores_cleanliness) %>%
  summarise(mn_reviews = mean(number_of_reviews),
            logmn_reviews = log(mn_reviews), n=n())

sumStats2 <- df_no_nulls %>% group_by(review_scores_location) %>%
  summarise(mn_reviews = mean(number_of_reviews),
            logmn_reviews = log(mn_reviews), n=n())

sumStats3 <- df_no_nulls %>% group_by(review_scores_value) %>%
  summarise(mn_reviews = mean(number_of_reviews),
            logmn_reviews = log(mn_reviews), n=n())

plot1 <- ggplot(sumStats, aes(x=review_scores_cleanliness, y=logmn_reviews)) +
  geom_point()+
  geom_smooth()+
  xlab("review_scores_cleanliness") +
  ylab("log(n_reviews)")

plot2 <- ggplot(sumStats2, aes(x=review_scores_location, y=logmn_reviews)) +
  geom_point()+
  geom_smooth()+
  xlab("review_scores_location") +
  ylab("log(n_reviews)")

plot3 <- ggplot(sumStats3, aes(x=review_scores_value, y=logmn_reviews)) +
  geom_point()+
  geom_smooth()+
  xlab("review_scores_value") +
  ylab("log(n_reviews)")

# Arrange the plots in a row using grid.arrange()
grid.arrange(plot1, plot2, plot3, ncol = 2)
```

The graphs above suggest that rate of increase in the number of reviews, log(n_reviews), follow a Poisson distribution since the rate is linear. The $log(\lambda)$ (where $\lambda$ = mean number of reviews per group of $\beta_i$) has a linear pattern which suggests that Poisson is appropriate.

#### Q 4.11.3.1 Airbnb in NYC - Offset

In general, when using a Poisson regression model, the variable to be offset should be a measure of exposure or effort. For this dataset, the variable to be offset would be a measure of the time that each rental unit is available for rent which is *days*.

Plotting $Y$, number of reviews against log(days).
```{r}
p1 <- ggplot(df, aes(x = days, y = number_of_reviews)) +
  geom_point() +
  xlab("days") +
  ylab("number of reviews")

p2 <- ggplot(df, aes(x=log(days), y=number_of_reviews)) +
  geom_point() +
  xlab("log(days)") +
  ylab("rate of number of reviews")

# Arrange the plots in a row using grid.arrange()
grid.arrange(p1, p2, ncol = 2)

# calculate correlation
cor(df$days, df$number_of_reviews)
```

The rate of increase in the number of reviews is exponential which suggets that log(days) may be needed to offset for the non-constant rate of number of reviews. The offset variable will be tested further in model building. 

#### Q 4.11.3.1 Airbnb in NYC - Model Building

Creating 2 models with the same predictors, but the first reduced model does not contain an offset variable of log(days).
```{r}
no_offset_model <- glm(number_of_reviews ~ review_scores_cleanliness + review_scores_location + review_scores_value,
                     data = df, family = poisson)
full_poisson_model <- glm(number_of_reviews ~ review_scores_cleanliness + review_scores_location + review_scores_value + offset(log(days)),
                     data = df, family = poisson)
```

```{r}

# data frame to summarize model comparisons
model_stats_df <- data.frame(
  Model = c("No Offset", "Offset"),
  Residual_Deviance = c(deviance(no_offset_model),deviance(full_poisson_model)),
  AIC = c(AIC(no_offset_model), AIC(full_poisson_model)),
  BIC = c(BIC(no_offset_model), BIC(full_poisson_model))
)

model_stats_df

```

The model without the offset has lower residual deviances, AIC, and BIC which suggests the model with the offset is not a better fit for the data than the model without the offset. This is because the residual deviance measures the lack of fit of the model to the data, and AIC/BIC provides a measure of the quality of the model in terms of goodness of fit and parsimony. A higher residual deviance and AIC both suggest a worse model fit, thus the model without offset will be used.

#### Q 4.11.3.1 Airbnb in NYC - Coefficient Interpretation
```{r}
# Obtaining coefficient estimates
coef_estimates <- coef(no_offset_model)

# Exponentiating coefficients to be able to interpret coefficients
# Because exponentiating the coefficient allows us 
# to obtain the multiplicative factor by which the mean count changes
exp_coef <- exp(coef_estimates)
round(exp_coef, 4)
```

Because we are working with categorical features (review_scores_cleanliness, review_scores_location, review_scores_values) each of their factors are considered a covariate in the Poisson models. Each coefficient was exponentiated to be able to interpret coefficients, which allows us to obtain the multiplicative factor by which the mean count changes. Some of the interpretations are translated below. 

For a **1 unit increase in the predictor variable**:

- *review_scores_cleanliness4*: the expected the response variable ($\lambda$ = estimated mean number of reviews) would decrease by a factor of approximately 0.7636 holding all other variables constant.
- *review_scores_location8*: the expected the response variable ($\lambda$ = estimated mean number of reviews) would increase by a factor of approximately 1.4741 holding all other variables constant.
- *review_scores_value9*: the expected the response variable ($\lambda$ = estimated mean number of reviews) would increase by a factor of approximately 6.7350 holding all other variables constant.

### Q 5.4.1

#### Q 5.4.1.g

Gamma (for fixed  r): Y = time spent waiting for the  rth event in a Poisson process with an average rate of $\lambda$ events per unit of time

a. Write the pmf or pdf in one-parameter exponential form, if possible.

$$ e^{log(y)r - rlog(\lambda)exp(\frac{-y}{\lambda})}  $$
b. Describe an example of a setting where this random variable might be used.

Modeling the time between successive arrivals of customers at a store. A Poisson process to model the arrival of customers, where $\lambda$ is the average number of customers that arrive in a unit of time. The random variable y would be the time that a customer spends in the line before being served, which follows the gamma distribution with parameters r and $\lambda$.

c. Identify the canonical link function.

- $a(y) = log(y)$
- $b(\lambda) = r$
- $c(\lambda) = rlog(\lambda)$
- $d(y) = exp(\frac{-y}{\lambda})$

d. Compute $\mu$ and $\sigma^2$, then compare with E(Y) and E(Y).

- $E(Y) = \frac{-\frac{r}{\lambda}}{1} \Rightarrow -\frac{r}{\lambda}$
- $\mu =\frac{r}{\lambda}$
- $Var(Y) = \frac{0 * \frac{r}{\lambda} - \frac{-r}{\lambda^2}}{1^2} \Rightarrow \frac{r}{\lambda^2}$
- $\sigma^2 =\frac{r}{\lambda^2}$


#### Q 5.4.1.i

a. Write the pmf or pdf in one-parameter exponential form, if possible.

$$  e^{ylog(1-p) + rlog(p) + log(\frac{\Gamma(y+r)}{\Gamma(r)})}  $$

b. Describe an example of a setting where this random variable might be used.

Can be used in sports analytics to model the number of games a team needs to win before reaching a certain number of losses. Each game has a probability of a win, p.

c. Identify the canonical link function.

- $a(y) = y$
- $b(p) = log(1-p)$
- $c(p) = rlog(p)$
- $d(y) = log(\frac{\Gamma(y+r)}{\Gamma(r)})$

d. Compute $\mu$ and $\sigma^2$, then compare with E(Y) and E(Y).

- $E(Y) = \frac{-\frac{r}{p}}{\frac{1}{1-p}} \Rightarrow \frac{r(1-p)}{p}$
- $\mu =\frac{r(1-p)}{p}$
- $Var(Y) = \frac{\frac{r}{p(1-p)^2} - \frac{r}{p^2(1-p)}}{(-\frac{1}{1-p})^3} \Rightarrow \frac{r(1-p)}{p^2}$
- $\sigma^2 =\frac{r(1-p)}{p^2}$

### Q 6.8.1.1

List the explanatory and response variable(s) for each research question.

#### Q 6.8.1.1 a
Are students with poor grades more likely to binge drink?
 
 - Explanatory Variable: Poor grades.
 - Response Variable: Binge drinker (binary).

#### Q 6.8.1.1 b
What is the chance you are accepted into medical school given your GPA and MCAT scores?

 - Explanatory Variable: GPA, MCAT Score.
 - Response Variable: Accepted to Medical School (binary).

#### Q 6.8.1.1 c
Is a single mom more likely to marry the baby’s father if she has a boy?

 - Explanatory Variable: Baby's gender.
 - Response Variable: Marry the baby’s father (binary).

#### Q 6.8.1.1 d
Are students participating in sports in college more or less likely to graduate?

 - Explanatory Variable: Participation in sports in college.
 - Response Variable: Graduate (binary).

#### Q 6.8.1.1 e
Is exposure to a particular chemical associated with a cancer diagnosis?

 - Explanatory Variable: Exposure to particular chemical.
 - Response Variable: Diagnosed with Cancer (binary).

### Q 6.8.2.2

Medical school admissions.

```{r}
df_med <- read.csv('./MedGPA.csv')
str(df_med)
```
```{r}
categorical_cols <- c('Accept', 'Acceptance', 'Sex')
df_med[categorical_cols] <- lapply(df_med[categorical_cols], as.factor)
```

#### Q 6.8.2.2 a

Compare the relative effects of improving your MCAT score versus improving your GPA on your odds of being accepted to medical school.

```{r}
# Group by Acceptance and show 5 number summary for MCAT & GPA
by(df_med[, c("MCAT", "GPA")], df_med$Acceptance, summary)
```

```{r}
# Fit a logistic regression model with both MCAT score and GPA as predictors
model <- glm(Acceptance ~ MCAT + GPA, data = df_med, family = binomial)

# View the model summary
summary(model)

# Exponentiate the coefficients
round(exp(coef(model)), 4)

round(exp(confint(model, level=0.95)), 4)
```

The above logistic regression model uses GPA and MCAT as its features for predicting acceptance of medical school.

 - The odds of getting accepted to medical school for one unit increase in GPA are 107.389 times the odds for a one unit decrease in GPA, holding all other predictor variables constant
 - The odds of getting accepted to medical school for one unit increase in MCAT score are 1.1788 times the odds for a one unit decrease in MCAT, holding all other predictor variables constant.
 - Both predictors have a positive association with getting accepted to medical school, however, GPA's odds ratio is 100x larger than MCAT Score, thus it is much stronger predictor according to this model.

Confidence Interval

- We are 95% confident that the interval (0.9743, 1.4672) contains
the true value odds ratio of MCAT and the interval (5.6895, 3913.2832) contains the true value odds ratio of MCAT in relation to getting accepted in medical school.

#### Q 6.8.2.2 b

After controlling for MCAT and GPA, is the number of applications related to odds of getting into medical school?

```{r}
# add Apps
model2 <- glm(Acceptance ~ MCAT + GPA + Apps, data = df_med, family = binomial)

summary(model2)
round(exp(coef(model2)), 4)

round(exp(confint(model2)), 4)
```

 The odds of getting accepted to medical school for one unit increase in number of applications are 1.0448 times the odds for a one unit decrease in number of applications, holding all other predictor variables constant. In other words, number of applications seems to be unrelated or there is little evidence that it is related to being accepted to medical school.
 
 Confidence Interval

- We are 95% confident that the interval (0.8993, 1.2190) contains
the true value odds ratio of number of applications in relation to getting accepted in medical school.

#### Q 6.8.2.2 c

Is one MCAT subscale more important than the others?

```{r}
# Remove overall MCAT and replace with subscore features
model3 <- glm(Acceptance ~ GPA  + BCPM + VR + PS + WS + BS, data = df_med, family = binomial)

summary(model3)
round(exp(coef(model3)), 4)

round(exp(confint(model3)), 4)
```

Yes BS, biological subscale is by far the most important according to this model because odds of getting accepted to medical school for one unit increase in BS are 5.3917 times the odds for a one unit decrease in BS. The next highest odd ratio is PS.

In order of importance
1. BS (Biological sciences)
2. PS (Physical sciences)
3. VR (Verbal Reasoning)

 Confidence Interval

- We are 95% confident that the interval (1.9540, 22.56040) contains
the true value odds ratio of BS in relation to getting accepted in medical school.


#### Q 6.8.2.2 d

Is there any evidence that the effect of MCAT score or GPA differs for males and females?

```{r}
reduced_model <- glm(Acceptance ~ MCAT + GPA + Sex, data = df_med, family = binomial)
full_model <- glm(Acceptance ~ MCAT*Sex + GPA*Sex + Sex, data = df_med, family = binomial)
# anova test
anova_res <- anova(reduced_model, full_model, test = "Chisq")
# View the model summary
anova_res
```

The full model above contains interaction terms to test if the affect of MCAT and GPA are influenced by gender. The anova test was performed and the full model has a p-value of 0.3397 At a 5% significance level (ɑ), we accept the null hypothesis because the p-value is greater than 0.05. We can conclude evidence that the reduced model is good enough, and that MCAT and GPA do not differ for males and females.

<!-- ```{r} -->
<!-- # Create a matrix for plots -->
<!-- # par(mfrow = c(1, 1)) -->

<!-- # Set up the plotting device with a larger width -->
<!-- #options(repr.plot.width = 8, repr.plot.height = 4) -->
<!-- # Set the margins for each individual plot -->
<!-- par(mar = c(4, 4, 2, 1),mfrow = c(2, 2)) -->

<!-- plot(df[['review_scores_cleanliness']], df$number_of_reviews, xlab='review_scores_cleanliness', ylab="n_reviews", type = "h") -->
<!-- plot(df[['review_scores_location']], df$number_of_reviews, xlab='review_scores_location', ylab="number_of_reviews", type = "h") -->
<!-- plot(df[['review_scores_value']], df$number_of_reviews, xlab='review_scores_value', ylab="number_of_reviews", type = "h") -->
<!-- ``` -->

<!-- To test out if the full model (model with the offset variable) is needed, the Likelihood Ratio Test (LRT) will be performed. Likelihood refers to the probability of observing the data given the model and the estimated parameters. The ANOVA (Analysis of variance) will be used to conduct the LRT. -->
<!-- ```{r} -->
<!-- # Perform the likelihood ratio test using anova() -->
<!-- anova(no_offset_model, full_poisson_model, test = "LRT") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # Calculate test statistic and p-value -->
<!-- test_stat <- 2 * (logLik(full_poisson_model) - logLik(no_offset_model)) -->
<!-- test_stat -->
<!-- #p_value <- pchisq(test_stat, df = 1, lower.tail = FALSE) -->
<!-- #p-value -->
<!-- ``` -->

<!-- The test statistic from the Log Likelihood test is too large to calculate a p-value from the Chi-square distribution and so a different method, the AIC metric, will be used to compare the models. -->

<!-- ```{r} -->
<!-- # Compute AIC and BIC -->
<!-- results <- data.frame( -->
<!--   model = c("no_offset_model", "full_poisson_model"), -->
<!--   df = AIC(no_offset_model, full_poisson_model)$df, -->
<!--   AIC = AIC(no_offset_model, full_poisson_model)$AIC, -->
<!--   BIC = BIC(no_offset_model, full_poisson_model)$BIC -->
<!-- ) -->

<!-- results -->
<!-- ``` -->

